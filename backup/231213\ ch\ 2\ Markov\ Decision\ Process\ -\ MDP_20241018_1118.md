#rl 


# MDP (Markov Decision Process) in general

state -> |agent| - action-> |environment|-> next reward, next state

## Markov property

> the future is independent of the past given the present


## Markov Reward Process (MRP)

MRP = MC (Markov Chain) + reward (gain)

Def: *reward function* (gain) includes a *discount factor* (like TVM, the future rewards are discounted)

$$
G_t=R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+2} + ...
$$

# Bellman equation

Def *value function*: expected return from starting in state $s$

$$
V(s)= E[G_t|s_t=s]
$$

Def: Bellman equation: current value function = immediate reward + discounted sum of expected future rewards (times with probabilities)

$$
V(s)=R(s) + \gamma \sum_{s'\in S} P(s'|s) V(s')
$$

part 1: immediate reward 
part 2: discounted sum of future reward

### Matrix form of Bellman Equation for MRP

$$
V = R + \gamma P V
$$

Analytic solution for value of MRP: $V = (I - \gamma P)^{-1} R$ 
only possible for small MRPs

iterative methods for large MRPs
- dynamic programming
- Monte Carlo evaluation
- temporal difference learning

# Markov Decision Process (MDP)

- MDP = MRP + D(decisions)
- MDP is MRP with decisions, $(S, A , P , R, \gamma)$
- include action: 
	- dynamics/transition model: $P(s_{t+1} = s' | s_t =s, a_t = a)$
	- reward function $R(s_t=s, a_t=a)=E[r_t|s_t=s, a_t=a]$

## Policy in MDP

- introduce a policy $\pi$ to MDP: ($S, A, P^\pi, R^\pi, \gamma$)

$$
P^\pi(s'|s)=\sum_{a\in A} \pi(a|s) P(s'|s, a)
$$

$$
R^\pi(s)=\sum_{a\in A} \pi(a|s)R(s,a)
$$


comparison of MP/MRP and MDP:

![](../../attachment/240108%20URC%20Adv%20proposal%20draft/231213%20ch%202%20Markov%20Decision%20Process%20-%20MDP_image_1.png)

*q function* or *action-value* function $q^\pi(s,a)$ 

$$
q^\pi(s,a) = E_\pi[G_t|s_t=s, A_t=a]
$$

*state-value* function $v^\pi(s)$ of an MDP

$$
v^\pi(s)=E_\pi[G_t| s_t=s]
$$

relation between $v^\pi(s)$ and $q^\pi(s,a)$

$$
v^\pi(s)=\sum_{a\in A} \pi(a|s) q^\pi(s,a)
$$

integrate with Bellman equation

$$
q^\pi(s,a) = R^a_s + \gamma \sum_{s'\in S} P(s'|s,a) v^\pi(s')
$$

$$
v^\pi(s)=\sum_{a\in A} \pi (a|s) (R(s,a) + \gamma \sum_{s'\in S}P(s'|s, a) v^\pi(s') )
$$

$$
q^\pi(s,a) = R(s,a) + \gamma \sum_{s'\in S} P(s'|s, a) \sum_{a'\in A} \pi(a'|s') q^\pi(s', a') 
$$

# Decision-making in MDP

- Prediction
	- input MDP or MRP with policy
	- output value function $v^\pi$ 
- control
	- input MDP
	- output optimal value function $v^*$ and optimal policy $\pi$ 
- prediction and control in MDP can be solved by dynamic programming


# Dynamic programming

- DP is a general solution method for programs with two properties
	- optimal substructure
	- overlapping subproblems
		- recursive
- MDP satisfy both properties
	- Bellman equation gives recursive decomposition
	- value function stores and reuses solutions


# policy evaluation on MDP

- object: evaluate a given policy $\pi$ for MDP
- output: the value function under policy $v^\pi$ 
- solution: iteration on Bellman expectation backup
- algorithm: synchronous backup

$$
v_{t+1}(s) = \sum_{a\in A} \pi(a|s) \left( R(s,a) +\gamma \sum_{s'\in S} P(s'|s,a)v_t(s') \right)
$$

- convergence: $v_1 \rightarrow v_2 \rightarrow ... \rightarrow v^\pi$

- Or in the form of MRP (marginalize $a$)

$$
v_{t+1}(s)= R^\pi(s) + \gamma P^\pi(s'|s)v_t(s')
$$

- [GridWorld: Dynamic Programming Demo](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html)

## optimal value function
- optimal state-value function $v^*(s)$ is the maximum value function overall policies
	- $v^*(s) = \underset{\pi}{\operatorname{max}} v^\pi(s)$ 
- optimal policy
	- $\pi^*(s)=\underset{\pi}{\operatorname{argmax}}  v^\pi(s)$
- An MDP is *solved* if we know the optimal value
- The optimal value function is *unique*, but could be *multiple* optimal policies (two actions have the same optimal value function)

## finding optimal policy
- by maximum the q function $q^*(s,a)$

$$
\pi^*(a|s) = 
\begin{cases}
1, & \text{if} a = \operatorname{argmax}_{a\in A} q^*(s,a)\\
0, & \text{overwise}
\end{cases}
$$

### policy search
- enumerate search the best policy
- other approaches such as policy iteration and value iteration are more efficient
	- policy iteration
		- policy evaluation + policy improvement
	- value iteration
		- finding optimal value function + one policy extraction


| Problem    | Bellman Equation             | Algorithm                   |
| ---------- | ---------------------------- | --------------------------- |
| Prediction | Bellman Expectation Equation | Iterative Policy Evaluation |
| Control    | Bellman Expectation Equation | Policy Iteration            |
| Control    | Bellman Optimality Equation  | Value Iteration             |

: Dynamic Programming Algorithms




# Summary

- MC: markov chain
- MRP = MC + R (rewards)
- MDP = MRP + D (decisions) = MC + R + D


















